import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
from scipy.signal import find_peaks
from tqdm import tqdm
import time

def extract_features(flattened_data):
    features = []
    for i in range(flattened_data.shape[0]):
        y_values = flattened_data[i]
        x_values = np.linspace(0, len(y_values) - 1, len(y_values))
        
        feature_dict = {}
        
        # Calculate slope
        slopes = np.gradient(y_values, x_values)
        feature_dict['mean_slope'] = np.mean(slopes)
        feature_dict['std_slope'] = np.std(slopes)
        
        # Slopes at the beginning and end of the plot
        feature_dict['start_slope'] = slopes[0]
        feature_dict['end_slope'] = slopes[-1]
        
        # Find peaks (local maxima)
        peaks, _ = find_peaks(y_values)
        feature_dict['num_peaks'] = len(peaks)
        feature_dict['mean_peak_height'] = np.mean(y_values[peaks]) if len(peaks) > 0 else 0
        
        # Find valleys (local minima)
        valleys, _ = find_peaks(-y_values)
        feature_dict['num_valleys'] = len(valleys)
        feature_dict['mean_valley_depth'] = np.mean(y_values[valleys]) if len(valleys) > 0 else 0
        
        # Calculate curvature
        curvature = np.gradient(slopes, x_values)
        feature_dict['mean_curvature'] = np.mean(curvature)
        feature_dict['std_curvature'] = np.std(curvature)
        
        # Symmetry
        feature_dict['symmetry'] = np.sum(np.abs(y_values - y_values[::-1])) / len(y_values)
        
        # Periodicity (for sine functions)
        autocorr = np.correlate(y_values, y_values, mode='full')
        feature_dict['periodicity'] = np.max(autocorr[len(autocorr)//2:])
        
        # Inflection points
        inflection_points = np.where(np.diff(np.sign(curvature)))[0]
        feature_dict['num_inflection_points'] = len(inflection_points)
        
        # Amplitude and frequency (for sine functions)
        if len(peaks) > 1:
            feature_dict['amplitude'] = (np.max(y_values[peaks]) - np.min(y_values[valleys])) / 2
            feature_dict['frequency'] = len(peaks) / (x_values[-1] - x_values[0])
        else:
            feature_dict['amplitude'] = 0
            feature_dict['frequency'] = 0
        
        # Exponential growth/decay
        if np.all(y_values > 0):
            feature_dict['exp_growth_rate'] = np.mean(np.diff(np.log(y_values)))
        else:
            feature_dict['exp_growth_rate'] = 0
        
        # Specific logic for different types of plots
        feature_dict['is_parabola'] = int(np.all(np.diff(np.sign(np.gradient(slopes))) != 0))  # Quadratic plot
        feature_dict['end_behavior'] = np.sign(y_values[0]) * np.sign(y_values[-1])  # Odd degree polynomial
        feature_dict['even_end_behavior'] = int(np.sign(y_values[0]) == np.sign(y_values[-1]))  # Even degree polynomial
        
        # Maximum number of critical points for polynomials
        feature_dict['num_critical_points'] = len(peaks) + len(valleys)
        
        # Absolute value function behavior
        if len(peaks) == 1 and len(valleys) == 0:
            feature_dict['abs_slope_diff'] = abs(slopes[0] - slopes[-1])
        else:
            feature_dict['abs_slope_diff'] = 0
        
        features.append(feature_dict)
    
    # Convert feature list to a numpy array
    feature_array = np.array([list(f.values()) for f in features])
    
    return feature_array

def load_data():
    # Load the data generated by graphs.py
    data = np.load('Matrices.npy')
    labels = np.load('Labels.npy')
    
    # Flatten the 2D matrices to 1D arrays for the model
    data = data.reshape(data.shape[0], -1)
    
    # Debugging statements
    print(f"Data shape: {data.shape}")
    print(f"Labels shape: {labels.shape}")
    print(f"Unique labels: {np.unique(labels)}")
    
    return data, labels

def train_advanced_model(data, labels):
    # Extract features from the flattened data
    feature_data = extract_features(data)
    
    X_train, X_test, y_train, y_test = train_test_split(feature_data, labels, test_size=0.2, random_state=42)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    
    # Add progress bar for the training process
    start_time = time.time()
    for i in tqdm(range(1, 101), desc="Training Progress"):
        model.n_estimators = i
        model.fit(X_train, y_train)
    end_time = time.time()
    print(f"Training time: {end_time - start_time} seconds")
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Advanced Model accuracy: {accuracy}")
    
    joblib.dump(model, 'advanced_graph_model.pkl')

if __name__ == "__main__":
    data, labels = load_data()
    train_advanced_model(data, labels)
